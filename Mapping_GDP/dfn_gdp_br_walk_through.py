# -*- coding: utf-8 -*-
"""DFN_GDP_BR_walk_through

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tSZp7gGY12arZ8KFPEa_UvopMqLBkLyl

------------------------------------------------------------------------------------------------------
Copyright (c) 2019 Rafael Guimaraes

This is part of a doctoral thesis in Economics at UFRGS/Brazil

*Chapter 2 - Mapping GDP with Deep Feedforward Networks*

*Guimaraes, R. & Ziegelmann, F. (2019)*

See full material at https://github.com/rrsguim/PhD_Economics

The code below, under MIT License, is an adaptation of

*Regression: Predict Fuel Efficiency, by TensorFlow Team*

https://www.tensorflow.org/tutorials/keras/basic_regression

-------------------------------------------------------------------------

# DFN GDP BR walk through
"""

# Use seaborn for pairplot
!pip install seaborn

from __future__ import absolute_import, division, print_function

import pathlib

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import numpy as np

import tensorflow as tf
from tensorflow import keras
from keras import layers
from tensorflow.python.keras.models import Sequential

print(tf.__version__)

"""# Get and organize the data

The csv file is available at https://github.com/rrsguim/PhD_Economics
"""

column_names = ['GDP','v1','v2','v3','v4','v5','v6','v7','v8','v9','v10','v11','v12','v13','v14','v15','v16','v17','v18','v19'] 
dataset = pd.read_csv("GDP_BR_19_1996_2017.csv", names=column_names,
                      na_values = "?", comment='\t',
                      sep=",", skipinitialspace=True)

dataset = dataset[84:] #because of NaN values in the first 83 lines
dataset.tail()

"""# Split data into train and test"""

train_dataset = dataset.sample(frac=0.7)#,random_state=0)
test_dataset = dataset.drop(train_dataset.index)

"""# Inspect the data

Have a quick look at the joint distribution of a few pairs of columns from the training set.
"""

sns.pairplot(train_dataset[["GDP","v1","v2","v3","v4","v5"]], diag_kind="kde")

"""Also look at the overall statistics:"""

train_stats = train_dataset.describe()
train_stats = train_stats.transpose()
train_stats

"""# Split features from labels

Separate the target value, or "label", from the features. This label is the value that we will train the model to predict.
"""

train_labels = train_dataset.pop('GDP')
test_labels = test_dataset.pop('GDP')

"""It is important to note that the split process nullifies the variables as time series. Due to our cross-sectional approach, this is acceptable and even desirable. The graph below shows the GDP before and after the split."""

axis = range(0,train_labels.shape[0])
gdp70 = dataset['GDP']
plt.plot(axis, gdp70[:126].T, '-', label='before')
plt.plot(axis, train_labels.T, '-', label='after')
plt.ylabel('GDP')
plt.legend()
plt.show()

"""# Build and inspect the model

Let's build our model. Here, we'll use a Sequential model with four densely connected hidden layers, and an output layer that returns a single, continuous value.
"""

def build_model():
    model = Sequential([
        keras.layers.Dense(150, kernel_regularizer=keras.regularizers.l1(0.001), 
                           activation=tf.nn.relu, input_shape=[len(train_dataset.keys())]),
        keras.layers.Dense(100, kernel_regularizer=keras.regularizers.l1(0.001), 
                           activation=tf.nn.relu),
        keras.layers.Dense(50, kernel_regularizer=keras.regularizers.l1(0.001), 
                           activation=tf.nn.relu),
        keras.layers.Dense(20, kernel_regularizer=keras.regularizers.l1(0.001), 
                           activation=tf.nn.relu),
        keras.layers.Dense(1)
    ])
    
    optimizer = tf.train.ProximalAdagradOptimizer(0.001)
    
    model.compile(loss='mse', 
                 optimizer=optimizer,
                 metrics=['mape'])
    return model

model = build_model()
model.summary()

"""# Train the model

Train the model for 2500 epochs, and record the training and validation accuracy in the history object.
"""

# Display training progress by printing a single dot for each completed epoch
class PrintDot(keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs):
    if epoch % 100 == 0: print('')
    print('.', end='')

EPOCHS = 2500

history = model.fit(
  train_dataset, train_labels,
  epochs=EPOCHS, validation_split = 0.3, verbose=0,
  callbacks=[PrintDot()])

"""Now we take a batch of 5 examples from the training data and call model.predict on it to see if the results are as expected in terms of shape and type."""

example_batch = train_dataset[:5]
example_result = model.predict(example_batch).flatten()
example_result

"""Visualize the model's training progress using the stats stored in the history object."""

hist = pd.DataFrame(history.history)
hist['epoch'] = history.epoch
hist.tail()

def plot_history(history):
  hist = pd.DataFrame(history.history)
  hist['epoch'] = history.epoch
  
  plt.figure()
  plt.xlabel('Epoch')
  plt.ylabel('Mean Abs Percentage Error [GDP]')
  plt.plot(hist['epoch'], hist['mean_absolute_percentage_error'],
           label='Train Error')
  plt.plot(hist['epoch'], hist['val_mean_absolute_percentage_error'],
           label = 'Val Error')
  plt.ylim([0,4])
  plt.legend()
  
  plt.show()


plot_history(history)

"""Let's see how well the model generalizes by using the test set, which we did not use when training the model. This tells us how well we can expect the model to predict when we use it in the real world."""

loss, mape = model.evaluate(test_dataset, test_labels, verbose=0)

print("Testing set MAPE: {:5.2f}".format(mape))

"""# Making prediction

Finally, predict MPG values using data in the testing set:
"""

test_predictions = model.predict(test_dataset).flatten()

plt.scatter(test_labels, test_predictions)
plt.xlabel('True Values [GDP]')
plt.ylabel('Predictions [GDP]')
plt.axis('equal')
plt.axis('square')
plt.xlim([100,plt.xlim()[1]])
plt.ylim([100,plt.ylim()[1]])
_ = plt.plot([0, 300], [0, 300])

"""Let's take a look at the error distribution."""

error = test_predictions - test_labels
plt.hist(error, bins = 25)
plt.xlabel("Prediction Error [GDP]")
_ = plt.ylabel("Count")

"""# GDP x DFN_GDP

Finally, we visually inspect the observed and the reconstructed GDP for all period. That is for visualization only, because just 30% comes from the testing set. For operational purposes, see the DFN_GDP_BR codes at https://github.com/rrsguim/PhD_Economics.
"""

dataset_copy = dataset
y = dataset_copy.pop('GDP')
x = dataset_copy

# "Predict" with all data
dfn_GDP = model.predict(x).flatten()
time_axis = range(0,y.shape[0]) 
plt.plot(time_axis, y.T, '-', label='GDP') #target
plt.plot(time_axis, dfn_GDP.T, '-', label='dfn_GDP') #model
plt.legend()
plt.show()

#export output to csv file
#import datetime
#now = datetime.datetime.now().strftime("%Y%m%d_%Hh%M")
#a = np.asarray(dfn_GDP)
#np.savetxt("DFN_GDP_BR_wt"+now+".csv", a, delimiter=",")